


     
Name : Abhishek G. Kale


ML Module Solution
Q-1. Imagine you have a dataset where you have different Instagram features like u sername , Caption , Hashtag , Followers , Time_Since_posted , and likes , now your task is to predict the number of likes and Time Since posted and the rest of the features are your input features. Now you have to build a model which can predict the number of likes and Time Since posted. Dataset This is the Dataset You can use this dataset for this question.


import pandas as pd
import numpy as np


df = pd.read_csv('/content/instagram_reach.csv')
     

df
     
Unnamed: 0	S.No	USERNAME	Caption	Followers	Hashtags	Time since posted	Likes
0	0	1	mikequindazzi	Who are #DataScientist and what do they do? >>...	1600	#MachineLearning #AI #DataAnalytics #DataScien...	11 hours	139
1	1	2	drgorillapaints	We all know where it’s going. We just have to ...	880	#deck .#mac #macintosh#sayhello #apple #steve...	2 hours	23
2	2	3	aitrading_official	Alexander Barinov: 4 years as CFO in multinati...	255	#whoiswho #aitrading #ai #aitradingteam#instat...	2 hours	25
3	3	4	opensourcedworkplace	sfad	340	#iot #cre#workplace #CDO #bigdata #technology#...	3 hours	49
4	4	5	crea.vision	Ever missed a call while your phone was chargi...	304	#instamachinelearning #instabigdata#instamarke...	3 hours	30
...	...	...	...	...	...	...	...	...
95	8	19	michaelgarza__	328 S. Wetherly Drive, Beverly Hills, CA 90212...	614	#beverlyhills #realestate#losangelesrealestate...	3 hours	31
96	9	21	dvlp_search	Credit @tristankappel To find more dvlp follow...	450	#workspace #work #developer#development #devel...	3 hours	42
97	10	22	ecom.space	We are coming up with the Best 21 Books that w...	182	#books #book #motivation #inspiration #life#bo...	3 hours	10
98	11	24	lb3enterprises	We’re only paid to move dirt once. It’s not ju...	2039	#heavyequipment #underconstruction#dozer #real...	3 hours	222
99	12	25	palmariusdev	Obtén tu tienda en línea ahora.	741	#marketing #programming#development #desarroll...	3 hours	109
100 rows × 8 columns


df.info()

     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 100 entries, 0 to 99
Data columns (total 8 columns):
 #   Column             Non-Null Count  Dtype 
---  ------             --------------  ----- 
 0   Unnamed: 0         100 non-null    int64 
 1   S.No               100 non-null    int64 
 2   USERNAME           100 non-null    object
 3   Caption            94 non-null     object
 4   Followers          100 non-null    int64 
 5   Hashtags           100 non-null    object
 6   Time since posted  100 non-null    object
 7   Likes              100 non-null    int64 
dtypes: int64(4), object(4)
memory usage: 6.4+ KB

df.isna().sum()
     
Unnamed: 0           0
S.No                 0
USERNAME             0
Caption              6
Followers            0
Hashtags             0
Time since posted    0
Likes                0
dtype: int64

df.columns

     
Index(['Unnamed: 0', 'S.No', 'USERNAME', 'Caption', 'Followers', 'Hashtags',
       'Time since posted', 'Likes'],
      dtype='object')

# dropping useless column
df = df.drop(['Unnamed: 0', 'S.No'],axis= 1)
     

df.columns
     
Index(['USERNAME', 'Caption', 'Followers', 'Hashtags', 'Time since posted',
       'Likes'],
      dtype='object')

X = df[['USERNAME', 'Caption', 'Followers', 'Hashtags']]
y_like = df['Likes']
y_time = df['Time since posted']

     

from sklearn.model_selection import train_test_split
X_train, X_test, Y_like_train, Y_like_test, Y_time_train, Y_time_test = train_test_split(X,y_like,y_time,random_state=21,test_size=0.2)
     

print(f"X_train shape = {X_train.shape}")
print(f"X_test shape = {X_test.shape}")
print(f"Y_like_train shape = {Y_like_train.shape}")
print(f"Y_like_test shape = {Y_like_test.shape}")
print(f"Y_time_train shape = {Y_time_train.shape}")
print(f"Y_time_test shape = {Y_time_test.shape}")
     
X_train shape = (80, 4)
X_test shape = (20, 4)
Y_like_train shape = (80,)
Y_like_test shape = (20,)
Y_time_train shape = (80,)
Y_time_test shape = (20,)

from sklearn.preprocessing import LabelEncoder

# Create an instance of LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the categorical variables
X_train_encoded = X_train.copy()
X_test_encoded = X_test.copy()


for col in X_train.columns:
    X_train_encoded[col] = label_encoder.fit_transform(X_train[col])
for col in X_test.columns:
    X_test_encoded[col] = label_encoder.fit_transform(X_test[col])
     

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
     

# For Likes prediction
lr = LinearRegression()
lr.fit(X_train_encoded, Y_like_train)
likes_predictions = lr.predict(X_test_encoded)

mse = mean_squared_error(Y_like_test, likes_predictions)
rmse = np.sqrt(mse)
r2 = r2_score(Y_like_test, likes_predictions)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared (R²):", r2)
     
Mean Squared Error (MSE): 6809.062884267752
Root Mean Squared Error (RMSE): 82.51704602242953
R-squared (R²): -0.9056777657870472

# Extract the numeric value from the 'time' column
Y_time_train = Y_time_train.str.extract('(\d+)', expand=False).astype(int)
Y_time_test = Y_time_test.str.extract('(\d+)', expand=False).astype(int)

# print(Y_time_train)
# print(Y_time_test)
     

lr = LinearRegression()
lr.fit(X_train_encoded, Y_time_train)
posted_time_prediction = lr.predict(X_test_encoded)

mse = mean_squared_error(Y_time_test, posted_time_prediction)
rmse = np.sqrt(mse)
r2 = r2_score(Y_like_test, posted_time_prediction)

print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("R-squared (R²):", r2)
     
Mean Squared Error (MSE): 20.800068621629787
Root Mean Squared Error (RMSE): 4.560709223534185
R-squared (R²): -0.7485752673511119


     
Q-2. Imagine you have a dataset where you have different features like Age , Gender , Height , Weight , BMI , and Blood Pressure and you have to classify the people into different classes like Normal , Overweight , Obesity , Underweight , and Extreme Obesity by using any 4 different classification algorithms. Now you have to build a model which can classify people into different classes. Dataset This is the Dataset You can use this dataset for this question.


## Import the necessary libraries:-
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')
     

# Load the dataset
data = pd.read_csv("/content/ObesityDataSet_raw_and_data_sinthetic.csv")
     

## Checking top 5 rows
data.head()
     
Gender	Age	Height	Weight	family_history_with_overweight	FAVC	FCVC	NCP	CAEC	SMOKE	CH2O	SCC	FAF	TUE	CALC	MTRANS	NObeyesdad
0	Female	21.0	1.62	64.0	yes	no	2.0	3.0	Sometimes	no	2.0	no	0.0	1.0	no	Public_Transportation	Normal_Weight
1	Female	21.0	1.52	56.0	yes	no	3.0	3.0	Sometimes	yes	3.0	yes	3.0	0.0	Sometimes	Public_Transportation	Normal_Weight
2	Male	23.0	1.80	77.0	yes	no	2.0	3.0	Sometimes	no	2.0	no	2.0	1.0	Frequently	Public_Transportation	Normal_Weight
3	Male	27.0	1.80	87.0	no	no	3.0	3.0	Sometimes	no	2.0	no	2.0	0.0	Frequently	Walking	Overweight_Level_I
4	Male	22.0	1.78	89.8	no	no	2.0	1.0	Sometimes	no	2.0	no	0.0	0.0	Sometimes	Public_Transportation	Overweight_Level_II

## Checking Rows & Columns Availabale in Dataset
data.shape
     
(2111, 17)

## Checking Details Information related with Dataset
data.info()

     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2111 entries, 0 to 2110
Data columns (total 17 columns):
 #   Column                          Non-Null Count  Dtype  
---  ------                          --------------  -----  
 0   Gender                          2111 non-null   object 
 1   Age                             2111 non-null   float64
 2   Height                          2111 non-null   float64
 3   Weight                          2111 non-null   float64
 4   family_history_with_overweight  2111 non-null   object 
 5   FAVC                            2111 non-null   object 
 6   FCVC                            2111 non-null   float64
 7   NCP                             2111 non-null   float64
 8   CAEC                            2111 non-null   object 
 9   SMOKE                           2111 non-null   object 
 10  CH2O                            2111 non-null   float64
 11  SCC                             2111 non-null   object 
 12  FAF                             2111 non-null   float64
 13  TUE                             2111 non-null   float64
 14  CALC                            2111 non-null   object 
 15  MTRANS                          2111 non-null   object 
 16  NObeyesdad                      2111 non-null   object 
dtypes: float64(8), object(9)
memory usage: 280.5+ KB

## Checking All Columns name present in dataset
data.columns
     
Index(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',
       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',
       'CALC', 'MTRANS', 'NObeyesdad'],
      dtype='object')

## Checking Statistical Analysis of Dataset
data.describe()

     
Age	Height	Weight	FCVC	NCP	CH2O	FAF	TUE
count	2111.000000	2111.000000	2111.000000	2111.000000	2111.000000	2111.000000	2111.000000	2111.000000
mean	24.312600	1.701677	86.586058	2.419043	2.685628	2.008011	1.010298	0.657866
std	6.345968	0.093305	26.191172	0.533927	0.778039	0.612953	0.850592	0.608927
min	14.000000	1.450000	39.000000	1.000000	1.000000	1.000000	0.000000	0.000000
25%	19.947192	1.630000	65.473343	2.000000	2.658738	1.584812	0.124505	0.000000
50%	22.777890	1.700499	83.000000	2.385502	3.000000	2.000000	1.000000	0.625350
75%	26.000000	1.768464	107.430682	3.000000	3.000000	2.477420	1.666678	1.000000
max	61.000000	1.980000	173.000000	3.000000	4.000000	3.000000	3.000000	2.000000

## Checking Information Related with Dataset
data.info()

     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2111 entries, 0 to 2110
Data columns (total 17 columns):
 #   Column                          Non-Null Count  Dtype  
---  ------                          --------------  -----  
 0   Gender                          2111 non-null   object 
 1   Age                             2111 non-null   float64
 2   Height                          2111 non-null   float64
 3   Weight                          2111 non-null   float64
 4   family_history_with_overweight  2111 non-null   object 
 5   FAVC                            2111 non-null   object 
 6   FCVC                            2111 non-null   float64
 7   NCP                             2111 non-null   float64
 8   CAEC                            2111 non-null   object 
 9   SMOKE                           2111 non-null   object 
 10  CH2O                            2111 non-null   float64
 11  SCC                             2111 non-null   object 
 12  FAF                             2111 non-null   float64
 13  TUE                             2111 non-null   float64
 14  CALC                            2111 non-null   object 
 15  MTRANS                          2111 non-null   object 
 16  NObeyesdad                      2111 non-null   object 
dtypes: float64(8), object(9)
memory usage: 280.5+ KB

## Checking All Columns Available in dataset
data.columns


     
Index(['Gender', 'Age', 'Height', 'Weight', 'family_history_with_overweight',
       'FAVC', 'FCVC', 'NCP', 'CAEC', 'SMOKE', 'CH2O', 'SCC', 'FAF', 'TUE',
       'CALC', 'MTRANS', 'NObeyesdad'],
      dtype='object')

# Preprocess the dataset
encoder = LabelEncoder()
data['Gender'] = encoder.fit_transform(data['Gender'])
data['family_history_with_overweight'] = encoder.fit_transform(data['family_history_with_overweight'])
data['FAVC'] = encoder.fit_transform(data['FAVC'])
data['CAEC'] = encoder.fit_transform(data['CAEC'])
data['SMOKE'] = encoder.fit_transform(data['SMOKE'])
data['SCC'] = encoder.fit_transform(data['SCC'])
data['CALC'] = encoder.fit_transform(data['CALC'])
data['MTRANS'] = encoder.fit_transform(data['MTRANS'])
data['NObeyesdad'] = encoder.fit_transform(data['NObeyesdad'])
     

## Checking Details Information related with Dataset
data.info()
     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 2111 entries, 0 to 2110
Data columns (total 17 columns):
 #   Column                          Non-Null Count  Dtype  
---  ------                          --------------  -----  
 0   Gender                          2111 non-null   int64  
 1   Age                             2111 non-null   float64
 2   Height                          2111 non-null   float64
 3   Weight                          2111 non-null   float64
 4   family_history_with_overweight  2111 non-null   int64  
 5   FAVC                            2111 non-null   int64  
 6   FCVC                            2111 non-null   float64
 7   NCP                             2111 non-null   float64
 8   CAEC                            2111 non-null   int64  
 9   SMOKE                           2111 non-null   int64  
 10  CH2O                            2111 non-null   float64
 11  SCC                             2111 non-null   int64  
 12  FAF                             2111 non-null   float64
 13  TUE                             2111 non-null   float64
 14  CALC                            2111 non-null   int64  
 15  MTRANS                          2111 non-null   int64  
 16  NObeyesdad                      2111 non-null   int64  
dtypes: float64(8), int64(9)
memory usage: 280.5 KB

# Split the dataset into features (X) and target (y)
X = data.drop('NObeyesdad', axis=1)
y = data['NObeyesdad']
     

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
     

# Decision Tree Classifier
dt_clf = DecisionTreeClassifier()
dt_clf.fit(X_train, y_train)
dt_predictions = dt_clf.predict(X_test)
     

# Logistic Regression Classifier
lr_clf = LogisticRegression()
lr_clf.fit(X_train, y_train)
lr_predictions = lr_clf.predict(X_test)
     

# Random Forest Classifier
rf_clf = RandomForestClassifier()
rf_clf.fit(X_train, y_train)
rf_predictions = rf_clf.predict(X_test)
     

# Support Vector Machine (SVM) Classifier
svm_clf = SVC()
svm_clf.fit(X_train, y_train)
svm_predictions = svm_clf.predict(X_test)
     

# Print classification reports for each classifier
print("Decision Tree Classifier:")
print(classification_report(y_test, dt_predictions))
     
Decision Tree Classifier:
              precision    recall  f1-score   support

           0       0.90      0.98      0.94        56
           1       0.88      0.85      0.87        62
           2       0.95      0.94      0.94        78
           3       0.96      0.95      0.96        58
           4       1.00      1.00      1.00        63
           5       0.91      0.91      0.91        56
           6       0.96      0.94      0.95        50

    accuracy                           0.94       423
   macro avg       0.94      0.94      0.94       423
weighted avg       0.94      0.94      0.94       423


# Print classification reports for each classifier
print("Logistic Regression Classifier:")
print(classification_report(y_test, lr_predictions))
     
Logistic Regression Classifier:
              precision    recall  f1-score   support

           0       0.74      0.93      0.83        56
           1       0.53      0.42      0.47        62
           2       0.58      0.60      0.59        78
           3       0.82      0.84      0.83        58
           4       0.90      1.00      0.95        63
           5       0.54      0.38      0.44        56
           6       0.35      0.38      0.37        50

    accuracy                           0.65       423
   macro avg       0.64      0.65      0.64       423
weighted avg       0.64      0.65      0.64       423


print("Random Forest Classifier:")
print(classification_report(y_test, rf_predictions))

     
Random Forest Classifier:
              precision    recall  f1-score   support

           0       0.98      0.96      0.97        56
           1       0.88      0.90      0.89        62
           2       0.99      0.96      0.97        78
           3       0.97      0.98      0.97        58
           4       1.00      1.00      1.00        63
           5       0.88      0.89      0.88        56
           6       0.98      0.96      0.97        50

    accuracy                           0.95       423
   macro avg       0.95      0.95      0.95       423
weighted avg       0.95      0.95      0.95       423


print("SVM Classifier:")
print(classification_report(y_test, svm_predictions))

     
SVM Classifier:
              precision    recall  f1-score   support

           0       0.71      0.88      0.78        56
           1       0.48      0.34      0.40        62
           2       0.65      0.33      0.44        78
           3       0.77      0.41      0.54        58
           4       0.56      1.00      0.72        63
           5       0.47      0.48      0.47        56
           6       0.43      0.58      0.49        50

    accuracy                           0.57       423
   macro avg       0.58      0.57      0.55       423
weighted avg       0.59      0.57      0.54       423



     
Question NO 3
Q-3. Imagine you have a dataset where you have different categories of data, Now you need to find the most similar data to the given data by using any 4 different similarity algorithms. Now you have to build a model which can find the most similar data to the given data. Dataset This is the Dataset You can use this dataset for this question.


import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from matplotlib import pyplot as plt # For plotting
from nltk.tokenize import word_tokenize # For tokenizing the words
from nltk.corpus import stopwords # To Remove the stop words
from collections import Counter # To build the category

from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize # To tokenize sentences
ps = PorterStemmer() # To perform stemming

import os
for dirname, _, filenames in os.walk('/content/News_Category_Dataset_v3.json'):
    for filename in filenames:
        print(os.path.join(dirname, filename))
     

# Converting the json to data frame 
base_df = pd.read_json('/content/News_Category_Dataset_v3.json', lines=True)
base_df.head(5)

# Dropping the other not relevant data
base_df = base_df.drop(['authors','link','date','short_description'], axis = 1)
base_df.head(3)
     
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-28-d966749c5f32> in <cell line: 2>()
      1 # Converting the json to data frame
----> 2 base_df = pd.read_json('/content/News_Category_Dataset_v3.json', lines=True)
      3 base_df.head(5)
      4 
      5 # Dropping the other not relevant data

/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    209                 else:
    210                     kwargs[new_arg_name] = new_arg_value
--> 211             return func(*args, **kwargs)
    212 
    213         return cast(F, wrapper)

/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    329                     stacklevel=find_stack_level(),
    330                 )
--> 331             return func(*args, **kwargs)
    332 
    333         # error: "Callable[[VarArg(Any), KwArg(Any)], Any]" has no

/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py in read_json(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)
    755 
    756     with json_reader:
--> 757         return json_reader.read()
    758 
    759 

/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py in read(self)
    911                 data = ensure_str(self.data)
    912                 data_lines = data.split("\n")
--> 913                 obj = self._get_object_parser(self._combine_lines(data_lines))
    914         else:
    915             obj = self._get_object_parser(self.data)

/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py in _get_object_parser(self, json)
    935         obj = None
    936         if typ == "frame":
--> 937             obj = FrameParser(json, **kwargs).parse()
    938 
    939         if typ == "series" or obj is None:

/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py in parse(self)
   1062             self._parse_numpy()
   1063         else:
-> 1064             self._parse_no_numpy()
   1065 
   1066         if self.obj is None:

/usr/local/lib/python3.10/dist-packages/pandas/io/json/_json.py in _parse_no_numpy(self)
   1319         if orient == "columns":
   1320             self.obj = DataFrame(
-> 1321                 loads(json, precise_float=self.precise_float), dtype=None
   1322             )
   1323         elif orient == "split":

ValueError: No ':' found when decoding object value

 
     
Q-4. Imagine you working as a sale manager now you need to predict the Revenue and whether that particular revenue is on the weekend or not and find the Informational_Duration using the Ensemble learning algorithm Dataset This is the Dataset You can use this dataset for this question


## Import the necessary libraries:-
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
     

# Load the dataset
data = pd.read_csv('/content/online_shoppers_intention.csv')
     

data.head()

     
Administrative	Administrative_Duration	Informational	Informational_Duration	ProductRelated	ProductRelated_Duration	BounceRates	ExitRates	PageValues	SpecialDay	Month	OperatingSystems	Browser	Region	TrafficType	VisitorType	Weekend	Revenue
0	0	0.0	0	0.0	1	0.000000	0.20	0.20	0.0	0.0	Feb	1	1	1	1	Returning_Visitor	False	False
1	0	0.0	0	0.0	2	64.000000	0.00	0.10	0.0	0.0	Feb	2	2	1	2	Returning_Visitor	False	False
2	0	0.0	0	0.0	1	0.000000	0.20	0.20	0.0	0.0	Feb	4	1	9	3	Returning_Visitor	False	False
3	0	0.0	0	0.0	2	2.666667	0.05	0.14	0.0	0.0	Feb	3	2	2	4	Returning_Visitor	False	False
4	0	0.0	0	0.0	10	627.500000	0.02	0.05	0.0	0.0	Feb	3	3	1	4	Returning_Visitor	True	False

## Checking All Rows & Columns Available in Dataset 
data.shape

     
(12330, 18)

## Checking Information Related with Dataset
data.info()
     
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 12330 entries, 0 to 12329
Data columns (total 18 columns):
 #   Column                   Non-Null Count  Dtype  
---  ------                   --------------  -----  
 0   Administrative           12330 non-null  int64  
 1   Administrative_Duration  12330 non-null  float64
 2   Informational            12330 non-null  int64  
 3   Informational_Duration   12330 non-null  float64
 4   ProductRelated           12330 non-null  int64  
 5   ProductRelated_Duration  12330 non-null  float64
 6   BounceRates              12330 non-null  float64
 7   ExitRates                12330 non-null  float64
 8   PageValues               12330 non-null  float64
 9   SpecialDay               12330 non-null  float64
 10  Month                    12330 non-null  object 
 11  OperatingSystems         12330 non-null  int64  
 12  Browser                  12330 non-null  int64  
 13  Region                   12330 non-null  int64  
 14  TrafficType              12330 non-null  int64  
 15  VisitorType              12330 non-null  object 
 16  Weekend                  12330 non-null  bool   
 17  Revenue                  12330 non-null  bool   
dtypes: bool(2), float64(7), int64(7), object(2)
memory usage: 1.5+ MB

# Convert target variable to categorical
data['Revenue'] = data['Revenue'].astype(str)

     

# Extract the relevant features for revenue prediction
features = data.drop(['Revenue'], axis=1)

     

# Convert weekend column to numerical values (0 for False, 1 for True)
features['Weekend'] = features['Weekend'].astype(int)

     

# Convert informational duration column to numerical values (0 for False, 1 for True)
features['Informational_Duration'] = features['Informational_Duration'].apply(lambda x: 1 if x > 0 else 0)
     

# Encode categorical features using one-hot encoding
features = pd.get_dummies(features)
     

# Extract the target variable (Revenue)
target = data['Revenue']
     

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
     

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
     

# Train the classifier
rf_classifier.fit(X_train, y_train)
     
RandomForestClassifier(random_state=42)
In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.
On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.

# Predict the revenue on the test set
y_pred = rf_classifier.predict(X_test)
     

# Calculate accuracy and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)

     

print("Accuracy:", accuracy)
print("Confusion Matrix:")
print(confusion)
     
Accuracy: 0.8961881589618816
Confusion Matrix:
[[1985   70]
 [ 186  225]]


     
Q-5. Uber is a taxi service provider as we know, we need to predict the high booking area using an Unsupervised algorithm and price for the location using a supervised algorithm and use some map function to display the data Dataset This is the Dataset You can use this dataset for this question.


# Required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

pd.options.display.max_rows = None
pd.options.display.max_columns = None
     

# Load the dataset
df= pd.read_csv("/content/rideshare_kaggle.csv")
     
---------------------------------------------------------------------------
ParserError                               Traceback (most recent call last)
<ipython-input-4-6983f41ae3c4> in <cell line: 2>()
      1 # Load the dataset
----> 2 df= pd.read_csv("/content/rideshare_kaggle.csv")

/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    209                 else:
    210                     kwargs[new_arg_name] = new_arg_value
--> 211             return func(*args, **kwargs)
    212 
    213         return cast(F, wrapper)

/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py in wrapper(*args, **kwargs)
    329                     stacklevel=find_stack_level(),
    330                 )
--> 331             return func(*args, **kwargs)
    332 
    333         # error: "Callable[[VarArg(Any), KwArg(Any)], Any]" has no

/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    948     kwds.update(kwds_defaults)
    949 
--> 950     return _read(filepath_or_buffer, kwds)
    951 
    952 

/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py in _read(filepath_or_buffer, kwds)
    609 
    610     with parser:
--> 611         return parser.read(nrows)
    612 
    613 

/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py in read(self, nrows)
   1776                     columns,
   1777                     col_dict,
-> 1778                 ) = self._engine.read(  # type: ignore[attr-defined]
   1779                     nrows
   1780                 )

/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py in read(self, nrows)
    228         try:
    229             if self.low_memory:
--> 230                 chunks = self._reader.read_low_memory(nrows)
    231                 # destructive to chunks
    232                 data = _concatenate_chunks(chunks)

/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader.read_low_memory()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/parsers.pyx in pandas._libs.parsers.raise_parser_error()

ParserError: Error tokenizing data. C error: EOF inside string starting at row 170092

df.head()
     
id	timestamp	hour	day	month	datetime	timezone	source	destination	cab_type	product_id	name	price	distance	surge_multiplier	latitude	longitude	temperature	apparentTemperature	short_summary	long_summary	precipIntensity	precipProbability	humidity	windSpeed	windGust	windGustTime	visibility	temperatureHigh	temperatureHighTime	temperatureLow	temperatureLowTime	apparentTemperatureHigh	apparentTemperatureHighTime	apparentTemperatureLow	apparentTemperatureLowTime	icon	dewPoint	pressure	windBearing	cloudCover	uvIndex	visibility.1	ozone	sunriseTime	sunsetTime	moonPhase	precipIntensityMax	uvIndexTime	temperatureMin	temperatureMinTime	temperatureMax	temperatureMaxTime	apparentTemperatureMin	apparentTemperatureMinTime	apparentTemperatureMax	apparentTemperatureMaxTime
0	424553bb-7174-41ea-aeb4-fe06d4f4b9d7	1.544953e+09	9	16	12	2018-12-16 09:30:07	America/New_York	Haymarket Square	North Station	Lyft	lyft_line	Shared	5.0	0.44	1.0	42.2148	-71.033	42.34	37.12	Mostly Cloudy	Rain throughout the day.	0.0000	0.0	0.68	8.66	9.17	1545015600	10.000	43.68	1544968800	34.19	1545048000	37.95	1544968800	27.39	1545044400	partly-cloudy-night	32.70	1021.98	57	0.72	0	10.000	303.8	1544962084	1544994864	0.30	0.1276	1544979600	39.89	1545012000	43.68	1544968800	33.73	1545012000	38.07	1544958000
1	4bd23055-6827-41c6-b23b-3c491f24e74d	1.543284e+09	2	27	11	2018-11-27 02:00:23	America/New_York	Haymarket Square	North Station	Lyft	lyft_premier	Lux	11.0	0.44	1.0	42.2148	-71.033	43.58	37.35	Rain	Rain until morning, starting again in the eve...	0.1299	1.0	0.94	11.98	11.98	1543291200	4.786	47.30	1543251600	42.10	1543298400	43.92	1543251600	36.20	1543291200	rain	41.83	1003.97	90	1.00	0	4.786	291.1	1543232969	1543266992	0.64	0.1300	1543251600	40.49	1543233600	47.30	1543251600	36.20	1543291200	43.92	1543251600
2	981a3613-77af-4620-a42a-0c0866077d1e	1.543367e+09	1	28	11	2018-11-28 01:00:22	America/New_York	Haymarket Square	North Station	Lyft	lyft	Lyft	7.0	0.44	1.0	42.2148	-71.033	38.33	32.93	Clear	Light rain in the morning.	0.0000	0.0	0.75	7.33	7.33	1543334400	10.000	47.55	1543320000	33.10	1543402800	44.12	1543320000	29.11	1543392000	clear-night	31.10	992.28	240	0.03	0	10.000	315.7	1543319437	1543353364	0.68	0.1064	1543338000	35.36	1543377600	47.55	1543320000	31.04	1543377600	44.12	1543320000
3	c2d88af2-d278-4bfd-a8d0-29ca77cc5512	1.543554e+09	4	30	11	2018-11-30 04:53:02	America/New_York	Haymarket Square	North Station	Lyft	lyft_luxsuv	Lux Black XL	26.0	0.44	1.0	42.2148	-71.033	34.38	29.63	Clear	Partly cloudy throughout the day.	0.0000	0.0	0.73	5.28	5.28	1543514400	10.000	45.03	1543510800	28.90	1543579200	38.53	1543510800	26.20	1543575600	clear-night	26.64	1013.73	310	0.00	0	10.000	291.1	1543492370	1543526114	0.75	0.0000	1543507200	34.67	1543550400	45.03	1543510800	30.30	1543550400	38.53	1543510800
4	e0126e1f-8ca9-4f2e-82b3-50505a09db9a	1.543463e+09	3	29	11	2018-11-29 03:49:20	America/New_York	Haymarket Square	North Station	Lyft	lyft_plus	Lyft XL	9.0	0.44	1.0	42.2148	-71.033	37.44	30.88	Partly Cloudy	Mostly cloudy throughout the day.	0.0000	0.0	0.70	9.14	9.14	1543446000	10.000	42.18	1543420800	36.71	1543478400	35.75	1543420800	30.29	1543460400	partly-cloudy-night	28.61	998.36	303	0.44	0	10.000	347.7	1543405904	1543439738	0.72	0.0001	1543420800	33.10	1543402800	42.18	1543420800	29.11	1543392000	35.75	1543420800

df['datetime']=pd.to_datetime(df['datetime'])
     

df.columns
     
Index(['id', 'timestamp', 'hour', 'day', 'month', 'datetime', 'timezone',
       'source', 'destination', 'cab_type', 'product_id', 'name', 'price',
       'distance', 'surge_multiplier', 'latitude', 'longitude', 'temperature',
       'apparentTemperature', 'short_summary', 'long_summary',
       'precipIntensity', 'precipProbability', 'humidity', 'windSpeed',
       'windGust', 'windGustTime', 'visibility', 'temperatureHigh',
       'temperatureHighTime', 'temperatureLow', 'temperatureLowTime',
       'apparentTemperatureHigh', 'apparentTemperatureHighTime',
       'apparentTemperatureLow', 'apparentTemperatureLowTime', 'icon',
       'dewPoint', 'pressure', 'windBearing', 'cloudCover', 'uvIndex',
       'visibility.1', 'ozone', 'sunriseTime', 'sunsetTime', 'moonPhase',
       'precipIntensityMax', 'uvIndexTime', 'temperatureMin',
       'temperatureMinTime', 'temperatureMax', 'temperatureMaxTime',
       'apparentTemperatureMin', 'apparentTemperatureMinTime',
       'apparentTemperatureMax', 'apparentTemperatureMaxTime'],
      dtype='object')

df.isnull().sum().sum()
     
55095

df.dropna(axis=0,inplace=True)

     


     


     
Q-6. Imagine you have a dataset where you have predicted loan Eligibility using any 4 different classification algorithms. Now you have to build a model which can predict loan Eligibility and you need to find the accuracy of the model and built-in docker and use some library to display that in frontend Dataset This is the Dataset You can use this dataset for this question.



     
Q-7. Imagine you have a dataset where you need to predict the Genres of Music using an Unsupervised algorithm and you need to find the accuracy of the model, built-in docker, and use some library to display that in frontend Dataset This is the Dataset You can use this dataset for this question.


## Required Libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from pandas.plotting import scatter_matrix
import seaborn as sns
import matplotlib.pyplot as plt
     

## Loading the Dataset
data_1 = pd.read_csv('/content/data.csv')
data_2 = pd.read_csv('/content/data_2genre.csv')

data = pd.concat([data_1, data_2])
     

## Checking top 5 row of Dataset
data.head()
     
filename	tempo	beats	chroma_stft	rmse	spectral_centroid	spectral_bandwidth	rolloff	zero_crossing_rate	mfcc1	mfcc2	mfcc3	mfcc4	mfcc5	mfcc6	mfcc7	mfcc8	mfcc9	mfcc10	mfcc11	mfcc12	mfcc13	mfcc14	mfcc15	mfcc16	mfcc17	mfcc18	mfcc19	mfcc20	label
0	blues.00081.au	103.359375	50	0.380260	0.248262	2116.942959	1956.611056	4196.107960	0.127272	-26.929785	107.334008	-46.809993	40.932634	-21.463776	24.816275	-18.945571	15.253459	-15.050104	12.262823	-15.234050	14.336612	-13.821769	7.562789	-6.181372	0.330165	-6.829571	0.965922	-7.570825	2.918987	blues
1	blues.00022.au	95.703125	44	0.306451	0.113475	1156.070496	1497.668176	2170.053545	0.058613	-233.860772	136.170239	3.289490	48.931193	8.699106	13.954752	5.359449	0.381334	-13.490265	16.871769	5.145258	-2.250578	3.959198	5.322555	0.812028	-1.107202	-4.556555	-2.436490	3.316913	-0.608485	blues
2	blues.00031.au	151.999081	75	0.253487	0.151571	1331.073970	1973.643437	2900.174130	0.042967	-221.802549	110.843070	18.620984	30.922996	3.291330	22.587447	-20.418303	12.120995	-26.347956	-4.356270	-26.406674	-13.037723	-12.652228	-1.821905	-7.260097	-6.660252	-14.682694	-11.719264	-11.025216	-13.387260	blues
3	blues.00012.au	184.570312	91	0.269320	0.119072	1361.045467	1567.804596	2739.625101	0.069124	-207.208080	132.799175	-15.438986	60.986727	0.725807	12.437828	1.183836	-1.540779	-17.888154	8.358496	-2.452068	-0.613248	0.384877	2.605128	-5.188924	-9.527455	-9.244394	-2.848274	-1.418707	-5.932607	blues
4	blues.00056.au	161.499023	74	0.391059	0.137728	1811.076084	2052.332563	3927.809582	0.075480	-145.434568	102.829023	-12.517677	56.993472	2.271347	26.591402	-4.274803	17.338809	-7.985593	15.235427	-6.934599	7.457218	-10.470444	-2.360483	-6.783623	2.671134	-4.760879	-0.949005	0.024832	-2.005315	blues
Here is a list of the genres in our dataframe, along with their counts:

data['label'].value_counts()
     
blues        100
classical    100
country      100
disco        100
hiphop       100
jazz         100
metal        100
pop          100
reggae       100
rock         100
1            100
2            100
Name: label, dtype: int64
It looks like there are some weird numerical values. By looking at the data I see that "1" corresponds to "pop" and the "2" corresponds to "classical". Let's change those.


data['label'] = data['label'].replace(to_replace={1: 'pop', 2: 'classical'})
     
Now we can see the true value counts


data['label'].value_counts()
     
classical    200
pop          200
blues        100
country      100
disco        100
hiphop       100
jazz         100
metal        100
reggae       100
rock         100
Name: label, dtype: int64
Next, I'll do some exploratory data analysis to see what kind of relationships we have between our features.

Tempo Distribution Here we can see the differences in tempo distribution between some different genres. Most have fairly normal distributions with peaks around 100 BPM. Disco is distinctly different, with a peak closer to 150 BPM. Classical and Jazz are the most diverse, having less prominent peaks and a wider spread of tempos.


plt.figure(figsize=(30,10))

sns.kdeplot(data=data.loc[data['label']=='jazz', 'tempo'], label="Jazz")
sns.kdeplot(data=data.loc[data['label']=='pop', 'tempo'], label="Pop")
sns.kdeplot(data=data.loc[data['label']=='classical', 'tempo'], label="Classical")
sns.kdeplot(data=data.loc[data['label']=='hiphop', 'tempo'], label="Hiphop")
sns.kdeplot(data=data.loc[data['label']=='disco', 'tempo'], label="Disco")
sns.kdeplot(data=data.loc[data['label']=='country', 'tempo'], label="Country")
sns.kdeplot(data=data.loc[data['label']=='rock', 'tempo'], label="Rock")
sns.kdeplot(data=data.loc[data['label']=='metal', 'tempo'], label="Metal")
sns.kdeplot(data=data.loc[data['label']=='reggae', 'tempo'], label="Reggae")
sns.kdeplot(data=data.loc[data['label']=='blues', 'tempo'], label="Blues")

plt.title("Distribution of tempos by genre", fontsize = 18)

plt.xlabel("Tempo", fontsize = 18)

plt.legend()
     
<matplotlib.legend.Legend at 0x7f571308d990>

Tempo Mean However, if we look at the average tempo of each we can see that they are all very similar


plt.figure(figsize=(30,10))

genres = data['label'].unique()

tempos = [ data[data['label']==x].tempo.mean() for x in genres ]

sns.barplot(x=genres, y=tempos, palette="deep")

plt.title("Average tempo by genre", fontsize = 18)

plt.xlabel('Genre', fontsize = 18)
plt.ylabel('Mean Tempo', fontsize = 18)
     
Text(0, 0.5, 'Mean Tempo')



     
Now I am going to look at some of the less intuitive features in the dataset. These features are more technical. I had to do some research to understand their meanings and implications, so I will explain them below.

Spectral Centroid
Spectral centroid is the average of frequencies weighted by amplitude, so a high spectral centroid implies that higher frequencies have higher amplitudes, or are more prominent, in this sample.

Spectral Centroid Distribution Songs in the classical, jazz, and country genres seem to trend toward lower spectral centroids, while pop, disco, hiphop, and metal songs tend to have higher centroids. It's possible that high spectral centroids could be correlated with catchy songs that grab your attention with high frequencies, while low spectral centroids correlate with low-toned, more relaxed music that is more common in classical, jazz, and country.

The classical and metal genres both have fairly low variance, implying that they are less diverse in terms of spectral centroids.

The distributions of classical and metal have very little overlap. We could discern between these two genres fairly accurately even if we only used this feature.


plt.figure(figsize=(30,10))

sns.kdeplot(data=data.loc[data['label']=='jazz', 'spectral_centroid'], label="Jazz")
sns.kdeplot(data=data.loc[data['label']=='pop', 'spectral_centroid'], label="Pop")
sns.kdeplot(data=data.loc[data['label']=='classical', 'spectral_centroid'], label="Classical")
sns.kdeplot(data=data.loc[data['label']=='hiphop', 'spectral_centroid'], label="Hiphop")
sns.kdeplot(data=data.loc[data['label']=='disco', 'spectral_centroid'], label="Disco")
sns.kdeplot(data=data.loc[data['label']=='country', 'spectral_centroid'], label="Country")
sns.kdeplot(data=data.loc[data['label']=='rock', 'spectral_centroid'], label="Rock")
sns.kdeplot(data=data.loc[data['label']=='metal', 'spectral_centroid'], label="Metal")
sns.kdeplot(data=data.loc[data['label']=='reggae', 'spectral_centroid'], label="Reggae")
sns.kdeplot(data=data.loc[data['label']=='blues', 'spectral_centroid'], label="Blues")

plt.title("Distribution of spectral centroids by genre", fontsize = 18)

plt.xlabel("Spectral Centroid", fontsize = 18)

plt.legend()
     
<matplotlib.legend.Legend at 0x7f57170610f0>

Spectral Centroid Mean As can be seen below, there is much more variance in the means of the spectral centroids than there was for tempo.


plt.figure(figsize=(30,10))

genres = data['label'].unique()

spectral_centroids = [ data[data['label']==x].spectral_centroid.mean() for x in genres ]

sns.barplot(x=genres, y=spectral_centroids, palette="deep")

plt.title("Average spectral centroid by genre", fontsize = 18)

plt.xlabel('Genre', fontsize = 18)
plt.ylabel('Mean Spectral Centroid', fontsize = 18)
     
Text(0, 0.5, 'Mean Spectral Centroid')

Spectral Bandwidth
Spectral bandwidth is the width of the frequency band for which the frequencies are at least half of the maximum amplitude. Basically, it shows us how wide the range of prominent frequencies is.

Spectral Bandwidth Distribution Interestingly, there are three very distinct peaks in this graph: classical, metal, and pop. Their distributions have relatively low variance, and they have little overlap with each other, meaning that this feature will be useful in distinguishing them.

Most classical songs have a smaller spectral bandwidth. This could be due to many classical songs being played by a single instrument, such as piano, limiting the tonal range.

Pop songs tend to have higher bandwidths. This may be because most pop songs include multiple instruments and vocal parts.


plt.figure(figsize=(30,10))

sns.kdeplot(data=data.loc[data['label']=='jazz', 'spectral_bandwidth'], label="Jazz")
sns.kdeplot(data=data.loc[data['label']=='pop', 'spectral_bandwidth'], label="Pop")
sns.kdeplot(data=data.loc[data['label']=='classical', 'spectral_bandwidth'], label="Classical")
sns.kdeplot(data=data.loc[data['label']=='hiphop', 'spectral_bandwidth'], label="Hiphop")
sns.kdeplot(data=data.loc[data['label']=='disco', 'spectral_bandwidth'], label="Disco")
sns.kdeplot(data=data.loc[data['label']=='country', 'spectral_bandwidth'], label="Country")
sns.kdeplot(data=data.loc[data['label']=='rock', 'spectral_bandwidth'], label="Rock")
sns.kdeplot(data=data.loc[data['label']=='metal', 'spectral_bandwidth'], label="Metal")
sns.kdeplot(data=data.loc[data['label']=='reggae', 'spectral_bandwidth'], label="Reggae")
sns.kdeplot(data=data.loc[data['label']=='blues', 'spectral_bandwidth'], label="Blues")

plt.title("Distribution of spectral bandwidth by genre", fontsize = 18)

plt.xlabel("Spectral Bandwidth", fontsize = 18)

plt.legend()
     
<matplotlib.legend.Legend at 0x7f57137ecfa0>

Spectral Bandwidth Mean The spectral bandwidth means look very similar to the spectral centroid means. This may indicate some kind of correlation.


plt.figure(figsize=(30,10))

genres = data['label'].unique()

spectral_bandwidths = [ data[data['label']==x].spectral_bandwidth.mean() for x in genres ]

sns.barplot(x=genres, y=spectral_bandwidths, palette="deep")

plt.title("Average spectral_bandwidth by genre", fontsize = 18)

plt.xlabel('Genre', fontsize = 18)
plt.ylabel('Mean Spectral Bandwidth', fontsize = 18)
     
Text(0, 0.5, 'Mean Spectral Bandwidth')

Rolloff
Rolloff is a term typically used to describe filters. It describes the steepness of the transition from the stop band to the pass band (the stop band includes the blocked frequencies, while the pass band includes the audible frequencies). A higher rolloff might indicate music that has less overtones (peripheral frequencies with lower amplitude), or that sounds more "crisp" and clean.

Rolloff Distribution The rolloff distributions looks almost the same as the spectral bandwidth distributions. This very likely indicates a correlation between the two.

Pop, disco, hiphop, and metal have high rolloff. This seems to support my theory about "crisp" sounding music.


plt.figure(figsize=(30,10))

sns.kdeplot(data=data.loc[data['label']=='jazz', 'rolloff'], label="Jazz")
sns.kdeplot(data=data.loc[data['label']=='pop', 'rolloff'], label="Pop")
sns.kdeplot(data=data.loc[data['label']=='classical', 'rolloff'], label="Classical")
sns.kdeplot(data=data.loc[data['label']=='hiphop', 'rolloff'], label="Hiphop")
sns.kdeplot(data=data.loc[data['label']=='disco', 'rolloff'], label="Disco")
sns.kdeplot(data=data.loc[data['label']=='country', 'rolloff'], label="Country")
sns.kdeplot(data=data.loc[data['label']=='rock', 'rolloff'], label="Rock")
sns.kdeplot(data=data.loc[data['label']=='metal', 'rolloff'], label="Metal")
sns.kdeplot(data=data.loc[data['label']=='reggae', 'rolloff'], label="Reggae")
sns.kdeplot(data=data.loc[data['label']=='blues', 'rolloff'], label="Blues")
plt.title("Distribution of rolloff by genre", fontsize = 18)

plt.xlabel("Rolloff", fontsize = 18)

plt.legend()
     
<matplotlib.legend.Legend at 0x7f5713925ab0>

Rolloff Means There is a lot of variance in the means of the rolloff. It also closely resembles the means of the spectral bandwidth.


plt.figure(figsize=(30,10))

genres = data['label'].unique()

rolloffs = [ data[data['label']==x].rolloff.mean() for x in genres ]

sns.barplot(x=genres, y=rolloffs, palette="deep")

plt.title("Average rolloff by genre", fontsize = 18)

plt.xlabel('Genre', fontsize = 18)
plt.ylabel('Rolloff', fontsize = 18)
     
Text(0, 0.5, 'Rolloff')

Feature Correlations This heatmap shows the correlations between all of the features. This quantifies how close they are to a perfect linear relationship.


correlations = data.corr()

fig, ax = plt.subplots(figsize=(30,30))
sns.heatmap(correlations, vmax=1.0, center=0, fmt='.2f',
            square=True, linewidths=.5, annot=True, cbar_kws={"shrink": .70})
plt.show();
     
<ipython-input-19-9555b368b39c>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
  correlations = data.corr()

Observation

This filtered list more clearly shows the features with the strongest positive correlations.

Here we can see that rolloff is strongly correlated with both spectral centroid and spectral bandwidth.

I am not sure what the difference between tempo and beats is, but there seems to be some minor discrepancy.

We can also see that there is also a fairly strong correlation between spectral bandwidth and centroid

I would have expected the correlation between zero crossing rate and spectral centroid to be higher, since they are both dependent on frequency.


c = data.corr()

s = c.unstack()
so = s.sort_values(kind="quicksort")

print(so[745:-28])
     
mfcc8               mfcc6                 0.843818
zero_crossing_rate  spectral_centroid     0.874095
spectral_centroid   zero_crossing_rate    0.874095
spectral_bandwidth  spectral_centroid     0.920961
spectral_centroid   spectral_bandwidth    0.920961
beats               tempo                 0.953903
tempo               beats                 0.953903
rolloff             spectral_bandwidth    0.964335
spectral_bandwidth  rolloff               0.964335
spectral_centroid   rolloff               0.982622
rolloff             spectral_centroid     0.982622
dtype: float64
<ipython-input-20-9a9d36ee8b50>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
  c = data.corr()
This one shows the features with the strongest negative correlations.

mfcc2 (the second coefficient of the Mel-frequency cepstrum, a mathematical representation of the sound) has a strong negative correlation with centroid, rolloff, and bandwidth.


c = data.corr()

s = c.unstack()
so = s.sort_values(kind="quicksort")

print(so[:10])
     
spectral_centroid   mfcc2                -0.946137
mfcc2               spectral_centroid    -0.946137
                    rolloff              -0.940227
rolloff             mfcc2                -0.940227
mfcc2               spectral_bandwidth   -0.906566
spectral_bandwidth  mfcc2                -0.906566
zero_crossing_rate  mfcc2                -0.775487
mfcc2               zero_crossing_rate   -0.775487
mfcc1               mfcc2                -0.689646
mfcc2               mfcc1                -0.689646
dtype: float64
<ipython-input-21-6eab77162e12>:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.
  c = data.corr()
Scatter Plots These scatter plots effectively visualize the relationships between the highly correlated variables.

Most notably, we can see that some variables have negative, non-linear relationships with mfcc2. It is hard to say why this is, because my understanding of Mel-frequency cepstrum is fairly weak.


attributes = ["mfcc2", "spectral_centroid",
              "spectral_bandwidth", "rolloff", "zero_crossing_rate" ]
sm = scatter_matrix(data[attributes], figsize=(20, 15), diagonal = "kde");

#Hide all ticks
[s.set_xticks(()) for s in sm.reshape(-1)];
[s.set_yticks(()) for s in sm.reshape(-1)];

for x in sm.ravel():
    x.set_xlabel(x.get_xlabel(), fontsize = 14)
    x.set_ylabel(x.get_ylabel(), fontsize = 14)
     



     
Q-8. Quora question pair similarity, you need to find the Similarity between two questions by mapping the words in the questions using TF-IDF, and using a supervised Algorithm you need to find the similarity between the questions. Dataset This is the Dataset You can use this dataset for this question.



     
Q-9. A cyber security agent wants to check the Microsoft Malware so need he came to you as a Machine learning Engineering with Data, You need to find the Malware using a supervised algorithm and you need to find the accuracy of the model. Dataset This is the Dataset You can use this dataset for this question.

An Ad- Agency analyzed a dataset of online ads and used a machine learning
model to predict whether a user would click on an ad or not. Dataset This is the Dataset You can use this dataset for this question.



     
