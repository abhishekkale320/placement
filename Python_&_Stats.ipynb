


     
Name : Abhishek G. Kale




     
Python Module Solution


     
Question 1: - Write a program that takes a string as input, and counts the frequency of each word in the string, there mightbe repeated characters in the string. Your task is to find the highest frequency and returns the length of the highest-frequency word.


def find_highest_frequency_word_length(string):
    # Split the string into words
    words = string.split()

    # Create a dictionary to store word frequencies
    word_freq = {}

    # Count the frequency of each word
    for word in words:
        word_freq[word] = word_freq.get(word, 0) + 1

    # Find the maximum frequency
    max_freq = max(word_freq.values())

    # Find the length of the word with the maximum frequency
    max_freq_word_length = max(len(word) for word, freq in word_freq.items() if freq == max_freq)

    return max_freq_word_length


# Example input
string = "write write write all the number from from from 1 to 100"

# Call the function and print the result
print(find_highest_frequency_word_length(string))

     
5


     
Question 2: - Consider a string to be valid if all characters of the string appear the same number of times. It is also valid if he can remove just one character at the index in the string, and the remaining characters will occur the same number of times. Given a string, determine if it is valid. If so, return YES , otherwise return NO .


def is_valid_string(s):
    # Create a dictionary to store character frequencies
    char_freq = {}

    # Count the frequency of each character
    for char in s:
        char_freq[char] = char_freq.get(char, 0) + 1

    # Check if all frequencies are the same
    if len(set(char_freq.values())) == 1:
        return "YES"

    # Check if removing one character makes all frequencies the same
    for char in char_freq:
        char_freq[char] -= 1
        new_freq = list(char_freq.values())

        if len(set(new_freq)) == 1:
            return "YES"

        char_freq[char] += 1

    return "NO"


# Example input 1
s1 = "abc"
# Call the function and print the result
print(is_valid_string(s1))  # Output: YES

# Example input 2
s2 = "abcc"
# Call the function and print the result
print(is_valid_string(s2))  # Output: NO

     
YES
YES
Question 3: - Write a program, which would download the data from the provided link, and then read the data and convert that into properly structured data and return it in Excel format.


import pandas as pd
import requests

def download_and_convert_data(link, output_file):
    # Download the data from the provided link
    response = requests.get(link)
    data = response.json()

    # Extract the necessary attributes from the data
    structured_data = []
    for pokemon in data["pokemon"]:
        attributes = {
            "id": pokemon["id"],
            "num": pokemon["num"],
            "name": pokemon["name"],
            "img": pokemon["img"],
            "type": ", ".join(pokemon["type"]),
            "height": pokemon["height"],
            "weight": pokemon["weight"],
            "candy": pokemon.get("candy", ""),
            "candy_count": pokemon.get("candy_count", 0),
            "egg": pokemon.get("egg", ""),
            "spawn_chance": pokemon.get("spawn_chance", 0),
            "avg_spawns": pokemon.get("avg_spawns", 0),
            "spawn_time": pokemon.get("spawn_time", ""),
            "multipliers": ", ".join(map(str, pokemon.get("multipliers", []))),
            "weakness": ", ".join(pokemon.get("weaknesses", [])),
            "next_evolution": ", ".join([evo["num"] + " - " + evo["name"] for evo in pokemon.get("next_evolution", [])]),
            "prev_evolution": ", ".join([evo["num"] + " - " + evo["name"] for evo in pokemon.get("prev_evolution", [])])
        }
        structured_data.append(attributes)

    # Convert the structured data to a DataFrame
    df = pd.DataFrame(structured_data)

    # Convert the DataFrame to an Excel file
    df.to_excel(output_file, index=False)

    print("Data conversion completed successfully!")



     

# Example usage
link = "https://raw.githubusercontent.com/Biuni/PokemonGO-Pokedex/master/pokedex.json"
output_file = "structured_data.xlsx"

download_and_convert_data(link, output_file)

     
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-9277d72fcc49> in <cell line: 5>()
      3 output_file = "structured_data.xlsx"
      4 
----> 5 download_and_convert_data(link, output_file)

<ipython-input-5-fdbcf014efc0> in download_and_convert_data(link, output_file)
     24             "avg_spawns": pokemon.get("avg_spawns", 0),
     25             "spawn_time": pokemon.get("spawn_time", ""),
---> 26             "multipliers": ", ".join(map(str, pokemon.get("multipliers", []))),
     27             "weakness": ", ".join(pokemon.get("weaknesses", [])),
     28             "next_evolution": ", ".join([evo["num"] + " - " + evo["name"] for evo in pokemon.get("next_evolution", [])]),

TypeError: 'NoneType' object is not iterable


     
Question 4 - Write a program to download the data from the link given below and then read the data and convert the into the proper structure and return it as a CSV file.


import pandas as pd
import requests

def download_and_convert_data(link, output_file):
    # Download the data from the provided link
    response = requests.get(link)
    data = response.json()

    # Convert the data to a DataFrame
    df = pd.DataFrame(data)

    # Convert the DataFrame to a CSV file
    df.to_csv(output_file, index=False)

    print("Data conversion completed successfully!")




     

# Example usage
link = "https://data.nasa.gov/resource/y77d-th95.json"
output_file = "structured_data.csv"

download_and_convert_data(link, output_file)
     
Data conversion completed successfully!


     
Question 5 - Write a program to download the data from the given API link and then extract the following data with proper formatting


import requests

def download_and_extract_data(link):
    # Download the data from the provided API link
    response = requests.get(link)
    data = response.json()

    # Extract the show details
    show_name = data["name"]
    show_language = data["language"]
    show_genre = ", ".join(data["genres"])
    show_status = data["status"]
    show_summary = data["summary"]

    # Extract the episode details
    episode_list = data["_embedded"]["episodes"]
    episode_data = []
    for episode in episode_list:
        episode_data.append({
            "Episode Number": episode["number"],
            "Episode Title": episode["name"],
            "Airdate": episode["airdate"]
        })

    # Print the extracted show details
    print("Show Details:")
    print("Name:", show_name)
    print("Language:", show_language)
    print("Genre:", show_genre)
    print("Status:", show_status)
    print("Summary:", show_summary)
    print()

    # Print the extracted episode details
    print("Episode Details:")
    for episode in episode_data:
        print("Episode Number:", episode["Episode Number"])
        print("Episode Title:", episode["Episode Title"])
        print("Airdate:", episode["Airdate"])
        print()



     

# Example usage
link = "http://api.tvmaze.com/singlesearch/shows?q=westworld&embed=episodes"

download_and_extract_data(link)

     
Show Details:
Name: Westworld
Language: English
Genre: Drama, Science-Fiction, Western
Status: Ended
Summary: <p><b>Westworld</b> is a dark odyssey about the dawn of artificial consciousness and the evolution of sin. Set at the intersection of the near future and the reimagined past, it explores a world in which every human appetite, no matter how noble or depraved, can be indulged.</p>

Episode Details:
Episode Number: 1
Episode Title: The Original
Airdate: 2016-10-02

Episode Number: 2
Episode Title: Chestnut
Airdate: 2016-10-09

Episode Number: 3
Episode Title: The Stray
Airdate: 2016-10-16

Episode Number: 4
Episode Title: Dissonance Theory
Airdate: 2016-10-23

Episode Number: 5
Episode Title: Contrapasso
Airdate: 2016-10-30

Episode Number: 6
Episode Title: The Adversary
Airdate: 2016-11-06

Episode Number: 7
Episode Title: Trompe L'Oeil
Airdate: 2016-11-13

Episode Number: 8
Episode Title: Trace Decay
Airdate: 2016-11-20

Episode Number: 9
Episode Title: The Well-Tempered Clavier
Airdate: 2016-11-27

Episode Number: 10
Episode Title: The Bicameral Mind
Airdate: 2016-12-04

Episode Number: 1
Episode Title: Journey Into Night
Airdate: 2018-04-22

Episode Number: 2
Episode Title: Reunion
Airdate: 2018-04-29

Episode Number: 3
Episode Title: Virtù e Fortuna
Airdate: 2018-05-06

Episode Number: 4
Episode Title: The Riddle of the Sphinx
Airdate: 2018-05-13

Episode Number: 5
Episode Title: Akane No Mai
Airdate: 2018-05-20

Episode Number: 6
Episode Title: Phase Space
Airdate: 2018-05-27

Episode Number: 7
Episode Title: Les Écorchés
Airdate: 2018-06-03

Episode Number: 8
Episode Title: Kiksuya
Airdate: 2018-06-10

Episode Number: 9
Episode Title: Vanishing Point
Airdate: 2018-06-17

Episode Number: 10
Episode Title: The Passenger
Airdate: 2018-06-24

Episode Number: 1
Episode Title: Parce Domine
Airdate: 2020-03-15

Episode Number: 2
Episode Title: The Winter Line
Airdate: 2020-03-22

Episode Number: 3
Episode Title: The Absence of Field
Airdate: 2020-03-29

Episode Number: 4
Episode Title: The Mother of Exiles
Airdate: 2020-04-05

Episode Number: 5
Episode Title: Genre
Airdate: 2020-04-12

Episode Number: 6
Episode Title: Decoherence
Airdate: 2020-04-19

Episode Number: 7
Episode Title: Passed Pawn
Airdate: 2020-04-26

Episode Number: 8
Episode Title: Crisis Theory
Airdate: 2020-05-03

Episode Number: 1
Episode Title: The Auguries
Airdate: 2022-06-26

Episode Number: 2
Episode Title: Well Enough Alone
Airdate: 2022-07-03

Episode Number: 3
Episode Title: Années Folles
Airdate: 2022-07-10

Episode Number: 4
Episode Title: Generation Loss
Airdate: 2022-07-17

Episode Number: 5
Episode Title: Zhuangzi
Airdate: 2022-07-24

Episode Number: 6
Episode Title: Fidelity
Airdate: 2022-07-31

Episode Number: 7
Episode Title: Metanoia
Airdate: 2022-08-07

Episode Number: 8
Episode Title: Que Será, Será
Airdate: 2022-08-14



     
Question 6 - Using the data from Question 3, write code to analyze the data and answer the following questions Note 1. Draw plots to demonstrate the analysis for the following questions for better visualizations.


import pandas as pd
import matplotlib.pyplot as plt

def analyze_pokemon_data():
    # Read the data from the provided link
    url = "https://raw.githubusercontent.com/Biuni/PokemonGO-Pokedex/master/pokedex.json"
    data = pd.read_json(url)

    # Get all Pokemons whose spawn rate is less than 5%
    spawn_rate_threshold = 0.05
    low_spawn_rate_pokemon = data[data["spawn_chance"] < spawn_rate_threshold]
    print("Pokemons with spawn rate less than 5%:")
    print(low_spawn_rate_pokemon[["name", "spawn_chance"]])
    print()

    # Get all Pokemons that have less than 4 weaknesses
    max_weaknesses = 4
    less_weaknesses_pokemon = data[data["weaknesses"].apply(len) < max_weaknesses]
    print("Pokemons with less than 4 weaknesses:")
    print(less_weaknesses_pokemon[["name", "weaknesses"]])
    print()

    # Get all Pokemons that have no multipliers at all
    no_multipliers_pokemon = data[data["multipliers"].apply(len) == 0]
    print("Pokemons with no multipliers:")
    print(no_multipliers_pokemon[["name", "multipliers"]])
    print()

    # Get all Pokemons that do not have more than 2 evolutions
    max_evolutions = 2
    less_evolutions_pokemon = data[data["next_evolution"].apply(lambda x: len(x) <= max_evolutions)]
    print("Pokemons with 2 or fewer evolutions:")
    print(less_evolutions_pokemon[["name", "next_evolution"]])
    print()

    # Get all Pokemons whose spawn time is less than 300 seconds
    max_spawn_time = "05:00"  # Convert 300 seconds to "minute:second" format
    data["spawn_time"] = pd.to_datetime(data["spawn_time"], format="%M:%S")
    less_spawn_time_pokemon = data[data["spawn_time"] < max_spawn_time]
    print("Pokemons with spawn time less than 300 seconds:")
    print(less_spawn_time_pokemon[["name", "spawn_time"]])
    print()

    # Get all Pokemons who have more than two types of capabilities
    min_capabilities = 2
    more_capabilities_pokemon = data[data["type"].apply(len) > min_capabilities]
    print("Pokemons with more than two types of capabilities:")
    print(more_capabilities_pokemon[["name", "type"]])
    print()

    # Draw plots for visualizations
    # Plot for spawn rate distribution
    plt.hist(data["spawn_chance"], bins=20)
    plt.xlabel("Spawn Rate")
    plt.ylabel("Frequency")
    plt.title("Spawn Rate Distribution")
    plt.show()

    # Plot for weaknesses count distribution
    data["weakness_count"] = data["weaknesses"].apply(len)
    plt.hist(data["weakness_count"], bins=range(0, 9))
    plt.xlabel("Number of Weaknesses")
    plt.ylabel("Frequency")
    plt.title("Number of Weaknesses Distribution")
    plt.show()


# Call the function to perform the analysis
analyze_pokemon_data()

     
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3801             try:
-> 3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'spawn_chance'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
<ipython-input-12-9afcb3c2b896> in <cell line: 69>()
     67 
     68 # Call the function to perform the analysis
---> 69 analyze_pokemon_data()

<ipython-input-12-9afcb3c2b896> in analyze_pokemon_data()
      9     # Get all Pokemons whose spawn rate is less than 5%
     10     spawn_rate_threshold = 0.05
---> 11     low_spawn_rate_pokemon = data[data["spawn_chance"] < spawn_rate_threshold]
     12     print("Pokemons with spawn rate less than 5%:")
     13     print(low_spawn_rate_pokemon[["name", "spawn_chance"]])

/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   3805             if self.columns.nlevels > 1:
   3806                 return self._getitem_multilevel(key)
-> 3807             indexer = self.columns.get_loc(key)
   3808             if is_integer(indexer):
   3809                 indexer = [indexer]

/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:
-> 3804                 raise KeyError(key) from err
   3805             except TypeError:
   3806                 # If we have a listlike key, _check_indexing_error will raise

KeyError: 'spawn_chance'


     
Question 7 - Using the data from Question 4, write code to analyze the data and answer the following questions Note -

Draw plots to demonstrate the analysis for the following questions for better visualizations
Write code comments wherever required for code understanding

import pandas as pd
import matplotlib.pyplot as plt

def analyze_meteorite_data():
    # Read the data from the provided link
    url = "https://data.nasa.gov/resource/y77d-th95.json"
    data = pd.read_json(url)

    # Get all the Earth meteorites that fell before the year 2000
    earth_meteorites_before_2000 = data[(data["reclat"].notnull()) & (data["year"].astype(int) < 2000)]
    print("Earth meteorites that fell before the year 2000:")
    print(earth_meteorites_before_2000[["name", "year"]])
    print()

    # Get all the Earth meteorites coordinates that fell before the year 1970
    earth_meteorites_coords_before_1970 = data[(data["reclat"].notnull()) & (data["year"].astype(int) < 1970)]
    print("Earth meteorites coordinates that fell before the year 1970:")
    print(earth_meteorites_coords_before_1970[["reclat", "reclong"]])
    print()

    # Get all the Earth meteorites whose mass was more than 10000kg (assuming mass is in kg)
    min_mass = 10000
    heavy_meteorites = data[(data["reclat"].notnull()) & (data["mass (g)"].astype(float) > min_mass)]
    print("Earth meteorites with mass more than 10000kg:")
    print(heavy_meteorites[["name", "mass (g)"]])
    print()

    # Draw plots for visualizations
    # Scatter plot for meteorite mass versus year
    plt.scatter(data["year"].astype(int), data["mass (g)"].astype(float))
    plt.xlabel("Year")
    plt.ylabel("Mass (g)")
    plt.title("Meteorite Mass versus Year")
    plt.show()

    # Bar plot for meteorite classification count
    classification_count = data["recclass"].value_counts().sort_values(ascending=False)
    classification_count.plot(kind="bar")
    plt.xlabel("Meteorite Classification")
    plt.ylabel("Count")
    plt.title("Meteorite Classification Count")
    plt.show()


# Call the function to perform the analysis
analyze_meteorite_data()

     
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-13-57c323b2fb64> in <cell line: 46>()
     44 
     45 # Call the function to perform the analysis
---> 46 analyze_meteorite_data()

<ipython-input-13-57c323b2fb64> in analyze_meteorite_data()
      8 
      9     # Get all the Earth meteorites that fell before the year 2000
---> 10     earth_meteorites_before_2000 = data[(data["reclat"].notnull()) & (data["year"].astype(int) < 2000)]
     11     print("Earth meteorites that fell before the year 2000:")
     12     print(earth_meteorites_before_2000[["name", "year"]])

/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py in astype(self, dtype, copy, errors)
   6238         else:
   6239             # else, only a single dtype is given
-> 6240             new_data = self._mgr.astype(dtype=dtype, copy=copy, errors=errors)
   6241             return self._constructor(new_data).__finalize__(self, method="astype")
   6242 

/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py in astype(self, dtype, copy, errors)
    446 
    447     def astype(self: T, dtype, copy: bool = False, errors: str = "raise") -> T:
--> 448         return self.apply("astype", dtype=dtype, copy=copy, errors=errors)
    449 
    450     def convert(

/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py in apply(self, f, align_keys, ignore_failures, **kwargs)
    350                     applied = b.apply(f, **kwargs)
    351                 else:
--> 352                     applied = getattr(b, f)(**kwargs)
    353             except (TypeError, NotImplementedError):
    354                 if not ignore_failures:

/usr/local/lib/python3.10/dist-packages/pandas/core/internals/blocks.py in astype(self, dtype, copy, errors)
    524         values = self.values
    525 
--> 526         new_values = astype_array_safe(values, dtype, copy=copy, errors=errors)
    527 
    528         new_values = maybe_coerce_values(new_values)

/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py in astype_array_safe(values, dtype, copy, errors)
    297 
    298     try:
--> 299         new_values = astype_array(values, dtype, copy=copy)
    300     except (ValueError, TypeError):
    301         # e.g. astype_nansafe can fail on object-dtype of strings

/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py in astype_array(values, dtype, copy)
    228 
    229     else:
--> 230         values = astype_nansafe(values, dtype, copy=copy)
    231 
    232     # in pandas we don't store numpy str dtypes, so convert to object

/usr/local/lib/python3.10/dist-packages/pandas/core/dtypes/astype.py in astype_nansafe(arr, dtype, copy, skipna)
    168     if copy or is_object_dtype(arr.dtype) or is_object_dtype(dtype):
    169         # Explicit copy, or required since NumPy can't view from / to object.
--> 170         return arr.astype(dtype, copy=True)
    171 
    172     return arr.astype(dtype, copy=copy)

ValueError: invalid literal for int() with base 10: '1880-01-01T00:00:00.000'


     
Question 8 - Using the data from Question 5, write code the analyze the data and answer the following questions Note -

Draw plots to demonstrate the analysis for the following questions and better visualizations
Write code comments wherever required for code understanding

import requests
import pandas as pd
import matplotlib.pyplot as plt

def analyze_tv_show_data():
    # Send a GET request to the API link and retrieve the data
    url = "http://api.tvmaze.com/singlesearch/shows?q=westworld&embed=episodes"
    response = requests.get(url)
    data = response.json()

    # Extract the episode data
    episodes = data["_embedded"]["episodes"]

    # Convert the episode data into a DataFrame
    df = pd.DataFrame(episodes)

    # Convert the airdate column to datetime format
    df["airdate"] = pd.to_datetime(df["airdate"])

    # Get all the overall ratings for each season and compare the ratings using plots
    season_ratings = df.groupby("season")["rating"].mean()
    print("Overall ratings for each season:")
    print(season_ratings)
    print()

    # Plot the ratings for all seasons
    season_ratings.plot(kind="bar")
    plt.xlabel("Season")
    plt.ylabel("Average Rating")
    plt.title("Average Rating per Season")
    plt.show()

    # Get all the episode names whose average rating is more than 8 for every season
    high_rated_episodes = df.groupby(["season", "name"]).filter(lambda x: x["rating"].mean() > 8)
    print("Episode names with average rating more than 8 for every season:")
    print(high_rated_episodes[["season", "name"]])
    print()

    # Get all the episode names that aired before May 2019
    episodes_before_may_2019 = df[df["airdate"] < pd.to_datetime("2019-05-01")]
    print("Episode names that aired before May 2019:")
    print(episodes_before_may_2019[["name", "airdate"]])
    print()

    # Get the episode name from each season with the highest and lowest rating
    highest_rated_episodes = df.groupby("season")["rating"].idxmax()
    lowest_rated_episodes = df.groupby("season")["rating"].idxmin()
    highest_rated_episodes_info = df.loc[highest_rated_episodes, ["season", "name", "rating"]]
    lowest_rated_episodes_info = df.loc[lowest_rated_episodes, ["season", "name", "rating"]]
    print("Episode with the highest rating per season:")
    print(highest_rated_episodes_info)
    print()
    print("Episode with the lowest rating per season:")
    print(lowest_rated_episodes_info)
    print()

    # Get the summary for the most popular (highest-rated) episode in every season
    most_popular_episodes = df.groupby("season")["rating"].idxmax()
    most_popular_episodes_info = df.loc[most_popular_episodes, ["season", "name", "summary"]]
    print("Summary for the most popular episode in every season:")
    print(most_popular_episodes_info)


# Call the function to perform the analysis
analyze_tv_show_data()

     
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py in array_func(values)
   1790             try:
-> 1791                 result = self.grouper._cython_operation(
   1792                     "aggregate",

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in _cython_operation(self, kind, values, how, axis, min_count, **kwargs)
   1038         ngroups = self.ngroups
-> 1039         return cy_op.cython_operation(
   1040             values=values,

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in cython_operation(self, values, axis, min_count, comp_ids, ngroups, **kwargs)
    707 
--> 708         return self._cython_op_ndim_compat(
    709             values,

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in _cython_op_ndim_compat(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)
    511                 result_mask = result_mask[None, :]
--> 512             res = self._call_cython_op(
    513                 values2d,

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in _call_cython_op(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)
    570         out_shape = self._get_output_shape(ngroups, values)
--> 571         func = self._get_cython_function(self.kind, self.how, values.dtype, is_numeric)
    572         values = self._get_cython_vals(values)

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in _get_cython_function(cls, kind, how, dtype, is_numeric)
    191                 # raise NotImplementedError here rather than TypeError later
--> 192                 raise NotImplementedError(
    193                     f"function is not implemented for this dtype: "

NotImplementedError: function is not implemented for this dtype: [how->mean,dtype->object]

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-14-24cf7119d018> in <cell line: 65>()
     63 
     64 # Call the function to perform the analysis
---> 65 analyze_tv_show_data()

<ipython-input-14-24cf7119d018> in analyze_tv_show_data()
     19 
     20     # Get all the overall ratings for each season and compare the ratings using plots
---> 21     season_ratings = df.groupby("season")["rating"].mean()
     22     print("Overall ratings for each season:")
     23     print(season_ratings)

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py in mean(self, numeric_only, engine, engine_kwargs)
   2181             return self._numba_agg_general(sliding_mean, engine_kwargs)
   2182         else:
-> 2183             result = self._cython_agg_general(
   2184                 "mean",
   2185                 alt=lambda x: Series(x).mean(numeric_only=numeric_only_bool),

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py in _cython_agg_general(self, how, alt, numeric_only, min_count, ignore_failures, **kwargs)
   1808         # TypeError -> we may have an exception in trying to aggregate
   1809         #  continue and exclude the block
-> 1810         new_mgr = data.grouped_reduce(array_func, ignore_failures=ignore_failures)
   1811 
   1812         if not is_ser and len(new_mgr) < orig_len:

/usr/local/lib/python3.10/dist-packages/pandas/core/internals/base.py in grouped_reduce(self, func, ignore_failures)
    197 
    198         arr = self.array
--> 199         res = func(arr)
    200         index = default_index(len(res))
    201 

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py in array_func(values)
   1802                 # try to python agg
   1803                 # TODO: shouldn't min_count matter?
-> 1804                 result = self._agg_py_fallback(values, ndim=data.ndim, alt=alt)
   1805 
   1806             return result

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py in _agg_py_fallback(self, values, ndim, alt)
   1743         #  should always be preserved by the implemented aggregations
   1744         # TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?
-> 1745         res_values = self.grouper.agg_series(ser, alt, preserve_dtype=True)
   1746 
   1747         if isinstance(values, Categorical):

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in agg_series(self, obj, func, preserve_dtype)
   1079 
   1080         else:
-> 1081             result = self._aggregate_series_pure_python(obj, func)
   1082 
   1083         npvalues = lib.maybe_convert_objects(result, try_float=False)

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/ops.py in _aggregate_series_pure_python(self, obj, func)
   1102 
   1103         for i, group in enumerate(splitter):
-> 1104             res = func(group)
   1105             res = libreduction.extract_result(res)
   1106 

/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/groupby.py in <lambda>(x)
   2183             result = self._cython_agg_general(
   2184                 "mean",
-> 2185                 alt=lambda x: Series(x).mean(numeric_only=numeric_only_bool),
   2186                 numeric_only=numeric_only,
   2187             )

/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py in mean(self, axis, skipna, level, numeric_only, **kwargs)
  11845             **kwargs,
  11846         ):
> 11847             return NDFrame.mean(self, axis, skipna, level, numeric_only, **kwargs)
  11848 
  11849         setattr(cls, "mean", mean)

/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py in mean(self, axis, skipna, level, numeric_only, **kwargs)
  11399         **kwargs,
  11400     ) -> Series | float:
> 11401         return self._stat_function(
  11402             "mean", nanops.nanmean, axis, skipna, level, numeric_only, **kwargs
  11403         )

/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py in _stat_function(self, name, func, axis, skipna, level, numeric_only, **kwargs)
  11351                 name, axis=axis, level=level, skipna=skipna, numeric_only=numeric_only
  11352             )
> 11353         return self._reduce(
  11354             func, name=name, axis=axis, skipna=skipna, numeric_only=numeric_only
  11355         )

/usr/local/lib/python3.10/dist-packages/pandas/core/series.py in _reduce(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)
   4814                 )
   4815             with np.errstate(all="ignore"):
-> 4816                 return op(delegate, skipna=skipna, **kwds)
   4817 
   4818     def _reindex_indexer(

/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py in _f(*args, **kwargs)
     91             try:
     92                 with np.errstate(invalid="ignore"):
---> 93                     return f(*args, **kwargs)
     94             except ValueError as e:
     95                 # we want to transform an object array

/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py in f(values, axis, skipna, **kwds)
    153                     result = alt(values, axis=axis, skipna=skipna, **kwds)
    154             else:
--> 155                 result = alt(values, axis=axis, skipna=skipna, **kwds)
    156 
    157             return result

/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py in new_func(values, axis, skipna, mask, **kwargs)
    416             mask = isna(values)
    417 
--> 418         result = func(values, axis=axis, skipna=skipna, mask=mask, **kwargs)
    419 
    420         if datetimelike:

/usr/local/lib/python3.10/dist-packages/pandas/core/nanops.py in nanmean(values, axis, skipna, mask)
    704 
    705     count = _get_counts(values.shape, mask, axis, dtype=dtype_count)
--> 706     the_sum = _ensure_numeric(values.sum(axis, dtype=dtype_sum))
    707 
    708     if axis is not None and getattr(the_sum, "ndim", False):

/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py in _sum(a, axis, dtype, out, keepdims, initial, where)
     46 def _sum(a, axis=None, dtype=None, out=None, keepdims=False,
     47          initial=_NoValue, where=True):
---> 48     return umr_sum(a, axis, dtype, out, keepdims, initial, where)
     49 
     50 def _prod(a, axis=None, dtype=None, out=None, keepdims=False,

TypeError: unsupported operand type(s) for +: 'dict' and 'dict'


     
Question 9 - Write a program to read the data from the following link, perform data analysis and answer the following questions Note -

Write code comments wherever required for code understanding
Link - https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD Insights to be drawn - ● Get all the cars and their types that do not qualify for clean alternative fuel vehicle ● Get all TESLA cars with the model year, and model type made in Bothell City. ● Get all the cars that have an electric range of more than 100, and were made after 2015 ● Draw plots to show the distribution between city and electric vehicle type


import pandas as pd
import matplotlib.pyplot as plt

def analyze_car_data():
    # Read the data from the provided link
    url = "https://data.wa.gov/api/views/f6w7-q2d2/rows.csv?accessType=DOWNLOAD"
    data = pd.read_csv(url)

    # Get all the cars and their types that do not qualify for clean alternative fuel vehicle
    non_clean_fuel_cars = data[data["Qualifies for Clean Alternative Fuel Vehicle"] == "No"]
    print("Cars and their types that do not qualify for clean alternative fuel vehicle:")
    print(non_clean_fuel_cars[["Make", "Vehicle Type"]])
    print()

    # Get all TESLA cars with the model year, and model type made in Bothell City
    tesla_cars_bothell = data[(data["Make"] == "TESLA") & (data["City"] == "BOTHELL")]
    print("TESLA cars with model year and model type made in Bothell City:")
    print(tesla_cars_bothell[["Model Year", "Model Type"]])
    print()

    # Get all the cars that have an electric range of more than 100 and were made after 2015
    electric_cars = data[(data["Electric Range"] > 100) & (data["Model Year"] > 2015)]
    print("Cars with electric range greater than 100 and made after 2015:")
    print(electric_cars[["Make", "Model Year", "Electric Range"]])
    print()

    # Draw plots to show the distribution between city and electric vehicle type
    plt.figure(figsize=(12, 6))
    plt.scatter(data["City"], data["Electric Vehicle Type"], alpha=0.5)
    plt.xlabel("City")
    plt.ylabel("Electric Vehicle Type")
    plt.title("Distribution of Electric Vehicle Types across Cities")
    plt.xticks(rotation=45)
    plt.show()


# Call the function to perform the analysis
analyze_car_data()

     
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3801             try:
-> 3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Qualifies for Clean Alternative Fuel Vehicle'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
<ipython-input-15-331bd672eb3e> in <cell line: 38>()
     36 
     37 # Call the function to perform the analysis
---> 38 analyze_car_data()

<ipython-input-15-331bd672eb3e> in analyze_car_data()
      8 
      9     # Get all the cars and their types that do not qualify for clean alternative fuel vehicle
---> 10     non_clean_fuel_cars = data[data["Qualifies for Clean Alternative Fuel Vehicle"] == "No"]
     11     print("Cars and their types that do not qualify for clean alternative fuel vehicle:")
     12     print(non_clean_fuel_cars[["Make", "Vehicle Type"]])

/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   3805             if self.columns.nlevels > 1:
   3806                 return self._getitem_multilevel(key)
-> 3807             indexer = self.columns.get_loc(key)
   3808             if is_integer(indexer):
   3809                 indexer = [indexer]

/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:
-> 3804                 raise KeyError(key) from err
   3805             except TypeError:
   3806                 # If we have a listlike key, _check_indexing_error will raise

KeyError: 'Qualifies for Clean Alternative Fuel Vehicle'
Question 10 - Write a program to count the number of verbs, nouns, pronouns, and adjectives in a given particular phrase or paragraph, and return their respective count as a dictionary.


import nltk
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

def count_pos_tags(text):
    # Tokenize the text into words
    words = word_tokenize(text)

    # Perform part-of-speech tagging on the words
    tagged_words = pos_tag(words)

    # Initialize counters for each part of speech
    verb_count = 0
    noun_count = 0
    pronoun_count = 0
    adjective_count = 0

    # Iterate over the tagged words and count the occurrences of each part of speech
    for word, tag in tagged_words:
        if tag.startswith('V'):  # Verbs
            verb_count += 1
        elif tag.startswith('N'):  # Nouns
            noun_count += 1
        elif tag.startswith('PR'):  # Pronouns
            pronoun_count += 1
        elif tag.startswith('J'):  # Adjectives
            adjective_count += 1

    # Create a dictionary with the counts
    pos_counts = {
        "Verbs": verb_count,
        "Nouns": noun_count,
        "Pronouns": pronoun_count,
        "Adjectives": adjective_count
    }

    return pos_counts



     

# Test the program
text = "The quick brown fox jumps over the lazy dog."
pos_counts = count_pos_tags(text)
print(pos_counts)

     
---------------------------------------------------------------------------
LookupError                               Traceback (most recent call last)
<ipython-input-17-0c02e2db87fb> in <cell line: 3>()
      1 # Test the program
      2 text = "The quick brown fox jumps over the lazy dog."
----> 3 pos_counts = count_pos_tags(text)
      4 print(pos_counts)

<ipython-input-16-4f66fcf89f1f> in count_pos_tags(text)
      5 def count_pos_tags(text):
      6     # Tokenize the text into words
----> 7     words = word_tokenize(text)
      8 
      9     # Perform part-of-speech tagging on the words

/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py in word_tokenize(text, language, preserve_line)
    127     :type preserve_line: bool
    128     """
--> 129     sentences = [text] if preserve_line else sent_tokenize(text, language)
    130     return [
    131         token for sent in sentences for token in _treebank_word_tokenizer.tokenize(sent)

/usr/local/lib/python3.10/dist-packages/nltk/tokenize/__init__.py in sent_tokenize(text, language)
    104     :param language: the model name in the Punkt corpus
    105     """
--> 106     tokenizer = load(f"tokenizers/punkt/{language}.pickle")
    107     return tokenizer.tokenize(text)
    108 

/usr/local/lib/python3.10/dist-packages/nltk/data.py in load(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)
    748 
    749     # Load the resource.
--> 750     opened_resource = _open(resource_url)
    751 
    752     if format == "raw":

/usr/local/lib/python3.10/dist-packages/nltk/data.py in _open(resource_url)
    874 
    875     if protocol is None or protocol.lower() == "nltk":
--> 876         return find(path_, path + [""]).open()
    877     elif protocol.lower() == "file":
    878         # urllib might not use mode='rb', so handle this one ourselves:

/usr/local/lib/python3.10/dist-packages/nltk/data.py in find(resource_name, paths)
    581     sep = "*" * 70
    582     resource_not_found = f"\n{sep}\n{msg}\n{sep}\n"
--> 583     raise LookupError(resource_not_found)
    584 
    585 

LookupError: 
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
  
  For more information see: https://www.nltk.org/data.html

  Attempted to load tokenizers/punkt/PY3/english.pickle

  Searched in:
    - '/root/nltk_data'
    - '/usr/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/share/nltk_data'
    - '/usr/local/share/nltk_data'
    - '/usr/lib/nltk_data'
    - '/usr/local/lib/nltk_data'
    - ''
**********************************************************************
Python Module assesment end

Statistics Module Solutions
Q-1. A university wants to understand the relationship between the SAT scores of its applicants and their college GPA. They collect data on 500 students, including their SAT scores (out of 1600) and their college GPA (on a 4.0 scale). They find that the correlation coefficient between SAT scores and college GPA is 0.7. What does this correlation coefficient indicate about the relationship between SAT scores and college GPA?


##Solution : -
     
The correlation coefficient between SAT scores and college GPA of 0.7 indicates a strong positive linear relationship between the two variables.

A correlation coefficient measures the strength and direction of the linear relationship between two variables. In this case, a correlation coefficient of 0.7 indicates a strong positive relationship, meaning that as SAT scores increase, college GPA tends to increase as well.

The positive sign of the correlation coefficient indicates that higher SAT scores are associated with higher college GPA. This means that students who perform well on the SAT tend to have higher GPAs in college, on average.

It's important to note that correlation does not imply causation. While a strong positive correlation suggests a relationship between SAT scores and college GPA, it does not necessarily imply that SAT scores directly cause higher GPAs. Other factors, such as study habits, motivation, and academic support, may also contribute to a student's college success.

Overall, a correlation coefficient of 0.7 indicates a strong positive relationship between SAT scores and college GPA in the given dataset of 500 students.



     
Q-2. Consider a dataset containing the heights (in centimeters) of 1000 individuals. The mean height is 170 cm with a standard deviation of 10 cm. The dataset is approximately normally distributed, and its skewness is approximately zero. Based on this information, answer the following questions: a. What percentage of individuals in the dataset have heights between 160 cm and 180 cm? b. If we randomly select 100 individuals from the dataset, what is the probability that their average height is greater than 175 cm? c. Assuming the dataset follows a normal distribution, what is the z-score corresponding to a height of 185 cm? d. We know that 5% of the dataset has heights below a certain value. What is the approximate height corresponding to this threshold? e. Calculate the coefficient of variation (CV) for the dataset. f. Calculate the skewness of the dataset and interpret the result.

Solution :-

a. To find the percentage of individuals in the dataset with heights between 160 cm and 180 cm, we need to calculate the z-scores for these heights and use the standard normal distribution table.

The z-score formula is given by: z = (x - μ) / σ

For 160 cm: z1 = (160 - 170) / 10 = -1

For 180 cm: z2 = (180 - 170) / 10 = 1

Using the standard normal distribution table or a calculator, we can find the area under the curve between z1 and z2. This area represents the percentage of individuals in the dataset with heights between 160 cm and 180 cm.

b. To calculate the probability that the average height of 100 randomly selected individuals is greater than 175 cm, we can use the Central Limit Theorem. The Central Limit Theorem states that the distribution of sample means approaches a normal distribution with a mean equal to the population mean and a standard deviation equal to the population standard deviation divided by the square root of the sample size.

Since the dataset is approximately normally distributed with a mean of 170 cm and a standard deviation of 10 cm, the sample mean will also have a normal distribution with a mean of 170 cm and a standard deviation of 10 cm divided by the square root of 100 (which is 10 / sqrt(100) = 1).

We can then calculate the z-score for a height of 175 cm using the formula: z = (x - μ) / σ

z = (175 - 170) / 1 = 5

Using the standard normal distribution table or a calculator, we can find the area to the right of z = 5. This area represents the probability that the average height of 100 individuals is greater than 175 cm.

c. To find the z-score corresponding to a height of 185 cm, we use the formula: z = (x - μ) / σ

z = (185 - 170) / 10 = 1.5

The z-score of 1.5 indicates that a height of 185 cm is 1.5 standard deviations above the mean height.

d. To find the approximate height corresponding to a threshold of 5% below the dataset, we can use the inverse of the cumulative distribution function (CDF) of the normal distribution.

Using the standard normal distribution table or a calculator, we can find the z-score corresponding to a cumulative probability of 5% (which is -1.645 approximately). We can then use the z-score formula to calculate the height:

x = μ + z * σ

x = 170 + (-1.645) * 10 = 153.55

The approximate height corresponding to the threshold of 5% is 153.55 cm.

e. The coefficient of variation (CV) is a measure of relative variability and is calculated as the ratio of the standard deviation to the mean, expressed as a percentage.

CV = (σ / μ) * 100

CV = (10 / 170) * 100 = 5.88%

The coefficient of variation for the dataset is approximately 5.88%.

f. The skewness of the dataset measures the asymmetry of the distribution. A skewness value of approximately zero indicates that the dataset is symmetrically distributed.

Skewness can be calculated using the formula:

skewness = (3 * (mean - median)) / standard deviation

Since the skewness is stated to be approximately zero, it implies that the dataset is roughly symmetric and does not have a significant tail on either side.

Q-3. Consider the ‘Blood Pressure Before’ and ‘Blood Pressure After’ columns from the data and calculate the following https://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp=share_ a. Measure the dispersion in both and interpret the results. b. Calculate mean and 5% confidence interval and plot it in a graph c. Calculate the Mean absolute deviation and Standard deviation and interpret the results. d. Calculate the correlation coefficient and check the significance of it at 1% level of significance.



     
Q-4. A group of 20 friends decide to play a game in which they each write a number between 1 and 20 on a slip of paper and put it into a hat. They then draw one slip of paper at random. What is the probability that the number on the slip of paper is a perfect square (i.e., 1, 4, 9, or 16)?

Solution : To calculate the probability that the number drawn from the hat is a perfect square (1, 4, 9, or 16), we need to determine the number of favorable outcomes (perfect squares) and the total number of possible outcomes.

Number of favorable outcomes: There are four perfect squares between 1 and 20: 1, 4, 9, and 16.

Total number of possible outcomes: Each friend writes a number between 1 and 20 on a slip of paper, so there are 20 possible numbers that could be drawn.

Therefore, the probability can be calculated as:

Probability = Number of favorable outcomes / Total number of possible outcomes

Probability = 4 / 20 = 0.2

So, the probability that the number drawn from the hat is a perfect square is 0.2 or 20%.



     
Q-5. A certain city has two taxi companies: Company A has 80% of the taxis and Company B has 20% of the taxis. Company A's taxis have a 95% success rate for picking up passengers on time, while Company B's taxis have a 90% success rate. If a randomly selected taxi is late, what is the probability that it belongs to Company A?

To solve this problem, we can use Bayes' theorem to calculate the probability that a randomly selected taxi belongs to Company A given that it is late.

Let's denote the following probabilities: P(A) = Probability of selecting a taxi from Company A = 0.8 (as Company A has 80% of the taxis) P(B) = Probability of selecting a taxi from Company B = 0.2 (as Company B has 20% of the taxis) P(L|A) = Probability of a taxi being late given that it belongs to Company A = 1 - 0.95 = 0.05 (as the success rate for on-time pick-up is 95%) P(L|B) = Probability of a taxi being late given that it belongs to Company B = 1 - 0.90 = 0.10 (as the success rate for on-time pick-up is 90%)

We need to find P(A|L), the probability that a randomly selected taxi belongs to Company A given that it is late.

According to Bayes' theorem: P(A|L) = (P(L|A) * P(A)) / P(L)

To calculate P(L), the probability that a taxi is late, we can use the law of total probability: P(L) = P(L|A) * P(A) + P(L|B) * P(B)

Now let's substitute the values and calculate the probabilities:

P(L) = (0.05 * 0.8) + (0.10 * 0.2) = 0.04 + 0.02 = 0.06

P(A|L) = (0.05 * 0.8) / 0.06 = 0.04 / 0.06 = 0.67

Therefore, the probability that a randomly selected taxi belongs to Company A given that it is late is 0.67 or 67%.



     
Q-6. A pharmaceutical company is developing a drug that is supposed to reduce blood pressure. They conduct a clinical trial with 100 patients and record their blood pressure before and after taking the drug. The company wants to know if the change in blood pressure follows a normal distribution.


import pandas as pd
import scipy.stats as stats
url = 'https://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp=sharing'
df = pd.read_csv(url)
change_bp = df['Blood Pressure After'] - df['Blood Pressure Before']
statistic, p_value = stats.shapiro(change_bp)
print('Test Statistic:', statistic)
print('p-value:', p_value)

     
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3801             try:
-> 3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Blood Pressure After'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
<ipython-input-20-459ac8c59f9d> in <cell line: 5>()
      3 url = 'https://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp=sharing'
      4 df = pd.read_csv(url)
----> 5 change_bp = df['Blood Pressure After'] - df['Blood Pressure Before']
      6 statistic, p_value = stats.shapiro(change_bp)
      7 print('Test Statistic:', statistic)

/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   3805             if self.columns.nlevels > 1:
   3806                 return self._getitem_multilevel(key)
-> 3807             indexer = self.columns.get_loc(key)
   3808             if is_integer(indexer):
   3809                 indexer = [indexer]

/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:
-> 3804                 raise KeyError(key) from err
   3805             except TypeError:
   3806                 # If we have a listlike key, _check_indexing_error will raise

KeyError: 'Blood Pressure After'


     
Q-7. The equations of two lines of regression, obtained in a correlation analysis between variables X and Y are as follows: and . 2𝑋 + 3 − 8 = 0 2𝑌 + 𝑋 − 5 = 0 The variance of 𝑋 = 4 Find the a. Variance of Y b. Coefficient of determination of C and Y c. Standard error of estimate of X on Y and of Y on X


import numpy as np

# Given equations of regression lines
equation1 = lambda x: (8 - 2*x) / 3
equation2 = lambda x: (5 - x) / 2

# Variance of X
var_X = 4

# a. Variance of Y
var_Y = (2/3)**2 * var_X
print("Variance of Y:", var_Y)

# b. Coefficient of determination (R^2) of X and Y
correlation_coefficient = np.sqrt(var_Y / var_X)  # Square root of R^2
coefficient_of_determination = correlation_coefficient**2
print("Coefficient of determination of X and Y:", coefficient_of_determination)

# c. Standard error of estimate of X on Y and of Y on X
# Additional information (residual standard deviation) is required to calculate the standard error of estimate.

     
Variance of Y: 1.7777777777777777
Coefficient of determination of X and Y: 0.4444444444444444


     
Q-8. The anxiety levels of 10 participants were measured before and after a new therapy. The scores are not normally distributed. Use the Wilcoxon signed-rank test to test whether the therapy had a significant effect on anxiety levels. The data is given below: Participant Before therapy After therapy Difference


from scipy import stats

# Data
before_therapy = [5, 7, 6, 8, 4, 9, 3, 6, 7, 5]
after_therapy = [4, 6, 5, 7, 3, 8, 2, 5, 6, 4]

# Calculate the differences
differences = [a - b for a, b in zip(after_therapy, before_therapy)]

# Perform the Wilcoxon signed-rank test
statistic, p_value = stats.wilcoxon(differences)

# Print the test results
print("Wilcoxon signed-rank test:")
print("Statistic:", statistic)
print("P-value:", p_value)

     
Wilcoxon signed-rank test:
Statistic: 0.0
P-value: 0.001953125
Q-10. A factory produces light bulbs, and the probability of a bulb being defective is 0.05. The factory produces a large batch of 500 light bulbs. a. What is the probability that exactly 20 bulbs are defective? b. What is the probability that at least 10 bulbs are defective? c. What is the probability that at max 15 bulbs are defective? d. On average, how many defective bulbs would you expect in a batch of 500?


from scipy.stats import binom

# Constants
n = 500  # Total number of bulbs
p = 0.05  # Probability of a bulb being defective

# a. Probability of exactly 20 defective bulbs
prob_20_defective = binom.pmf(20, n, p)
print("Probability of exactly 20 defective bulbs:", prob_20_defective)

# b. Probability of at least 10 defective bulbs
prob_at_least_10_defective = 1 - binom.cdf(9, n, p)
print("Probability of at least 10 defective bulbs:", prob_at_least_10_defective)

# c. Probability of at most 15 defective bulbs
prob_at_most_15_defective = binom.cdf(15, n, p)
print("Probability of at most 15 defective bulbs:", prob_at_most_15_defective)

# d. Expected number of defective bulbs
expected_defective = n * p
print("Expected number of defective bulbs:", expected_defective)


     
Probability of exactly 20 defective bulbs: 0.051616192536641056
Probability of at least 10 defective bulbs: 0.9998316463654902
Probability of at most 15 defective bulbs: 0.01985837716300623
Expected number of defective bulbs: 25.0
Q-11. Given the data of a feature contributing to different classes


import pandas as pd
import scipy.stats as stats

# Load the data
data_url = 'https://drive.google.com/file/d/1mCjtYHiX--mMUjicuaP2gH3k-SnFxt8Y/view?usp=sharing'
data = pd.read_csv(data_url)

# a. Check whether the distribution of all the classes are the same or not
# Perform one-way ANOVA test
p_value = stats.f_oneway(data['Class1'], data['Class2'], data['Class3']).pvalue
if p_value < 0.05:
    print("The distribution of the classes is not the same.")
else:
    print("The distribution of the classes is the same.")

# b. Check for the equality of variance
# Perform Levene's test for equality of variances
p_value = stats.levene(data['Class1'], data['Class2'], data['Class3']).pvalue
if p_value < 0.05:
    print("The variances of the classes are not equal.")
else:
    print("The variances of the classes are equal.")

# c. Perform LDA and QDA and compare their performance
# Implement LDA and QDA classification models and evaluate their performance

# d. Check the equality of mean between all the classes
# Perform one-way ANOVA test
p_value = stats.f_oneway(data['Class1'], data['Class2'], data['Class3']).pvalue
if p_value < 0.05:
    print("The means of the classes are not equal.")
else:
    print("The means of the classes are equal.")

     
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3801             try:
-> 3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'Class1'

The above exception was the direct cause of the following exception:

KeyError                                  Traceback (most recent call last)
<ipython-input-24-3680e4d12033> in <cell line: 10>()
      8 # a. Check whether the distribution of all the classes are the same or not
      9 # Perform one-way ANOVA test
---> 10 p_value = stats.f_oneway(data['Class1'], data['Class2'], data['Class3']).pvalue
     11 if p_value < 0.05:
     12     print("The distribution of the classes is not the same.")

/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py in __getitem__(self, key)
   3805             if self.columns.nlevels > 1:
   3806                 return self._getitem_multilevel(key)
-> 3807             indexer = self.columns.get_loc(key)
   3808             if is_integer(indexer):
   3809                 indexer = [indexer]

/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   3802                 return self._engine.get_loc(casted_key)
   3803             except KeyError as err:
-> 3804                 raise KeyError(key) from err
   3805             except TypeError:
   3806                 # If we have a listlike key, _check_indexing_error will raise

KeyError: 'Class1'
Q-12. A pharmaceutical company develops a new drug and wants to compare its effectiveness against a standard drug for treating a particular condition. They conduct a study with two groups: Group A receives the new drug, and Group B receives the standard drug. The company measures the improvement in a specific symptom for both groups after a 4-week treatment period. a. The company collects data from 30 patients in each group and calculates the mean improvement score and the standard deviation of improvement for each group. The mean improvement score for Group A is 2.5 with a standard deviation of 0.8, while the mean improvement score for Group B is 2.2 with a standard deviation of 0.6. Conduct a t-test to determine if there is a significant difference in the mean improvement scores between the two groups. Use a significance level of 0.05. b. Based on the t-test results, state whether the null hypothesis should be rejected or not. Provide a conclusion in the context of the study.


import scipy.stats as stats

# Define the data for Group A and Group B
mean_a = 2.5
std_a = 0.8
n_a = 30

mean_b = 2.2
std_b = 0.6
n_b = 30

# Perform independent t-test
t_statistic, p_value = stats.ttest_ind_from_stats(mean_a, std_a, n_a, mean_b, std_b, n_b)

# Compare p-value with significance level
alpha = 0.05
if p_value < alpha:
    print("There is a significant difference in the mean improvement scores between Group A and Group B.")
    print("Null hypothesis should be rejected.")
else:
    print("There is no significant difference in the mean improvement scores between Group A and Group B.")
    print("Null hypothesis should not be rejected.")

     
There is no significant difference in the mean improvement scores between Group A and Group B.
Null hypothesis should not be rejected.
